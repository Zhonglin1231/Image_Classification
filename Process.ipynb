{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearnex import patch_sklearn, unpatch_sklearn\n",
    "patch_sklearn()\n",
    "\n",
    "import argparse\n",
    "import random\n",
    "import math\n",
    "import csv\n",
    "import pickle\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "from skimage import io, color\n",
    "from skimage.feature import hog\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "from scipy.signal import convolve2d\n",
    "\n",
    "from skimage.feature import local_binary_pattern\n",
    "\n",
    "\n",
    "# from skimage.color import rgb2gray\n",
    "# import imageio\n",
    "import skimage.feature\n",
    "\n",
    "# from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.ensemble import VotingClassifier\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "import pywt\n",
    "\n",
    "import mahotas\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Def Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def predict(self, img):\n",
    "        pass\n",
    "\n",
    "    def featureExtract(self, imgs):\n",
    "        feature = []\n",
    "        for i in tqdm(range(0, imgs.shape[0])):\n",
    "            dense_feat = self.predict(imgs[i])\n",
    "            feature.append(dense_feat)\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(feature)\n",
    "        normalized_feature = scaler.transform(feature)\n",
    "\n",
    "        return np.array(normalized_feature)\n",
    "    \n",
    "\n",
    "class Color(FeatureExtractor):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def predict(self, img):\n",
    "        # img = img.reshape(img.shape[0] * img.shape[1], -1)\n",
    "        # return np.mean(img, axis=0)\n",
    "    \n",
    "        pixels = np.array(img)\n",
    "\n",
    "        # Quantize colors to, say, 64 colors\n",
    "        # Reduce each color channel into 4 bins (4*4*4 = 64 colors)\n",
    "        pixels_quantized = (pixels // 64) * 64\n",
    "\n",
    "        # Create histogram\n",
    "        hist, _ = np.histogramdd(pixels_quantized.reshape(-1, 3), bins=(4, 4, 4), range=((0, 256), (0, 256), (0, 256)))\n",
    "\n",
    "        # Flatten histogram to use as a feature vector\n",
    "        feature_vector = hist.flatten()\n",
    "\n",
    "        # Normalize the histogram\n",
    "        feature_vector /= feature_vector.sum()\n",
    "\n",
    "        return feature_vector\n",
    "\n",
    "class Raw(FeatureExtractor):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def predict(self, img):\n",
    "        return img.flatten()\n",
    "    \n",
    "\n",
    "class SmoothRaw(FeatureExtractor):\n",
    "\n",
    "    def __init__(self, size=16):\n",
    "        super().__init__()\n",
    "        self.size = size\n",
    "\n",
    "    def predict(self, img):\n",
    "        b, g, r = cv2.split(img)\n",
    "        gray_image = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        resized_grey_image = cv2.resize(gray_image, (self.size, self.size), interpolation=cv2.INTER_AREA)\n",
    "        resized_b = cv2.resize(b, (self.size, self.size), interpolation=cv2.INTER_AREA)\n",
    "        resized_g = cv2.resize(g, (self.size, self.size), interpolation=cv2.INTER_AREA)\n",
    "        resized_r = cv2.resize(r, (self.size, self.size), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "        return np.concatenate((resized_grey_image, resized_b, resized_g, resized_r)).flatten()\n",
    "\n",
    "\n",
    "class cv2HOG(FeatureExtractor):\n",
    "    def __init__(self, win_size=(32, 32), block_size=(16, 16), block_stride=(8, 8), cell_size=(8, 8), num_bins=20):\n",
    "        super().__init__()\n",
    "        self.win_size = win_size\n",
    "        self.block_size = block_size\n",
    "        self.block_stride = block_stride\n",
    "        self.cell_size = cell_size\n",
    "        self.num_bins = num_bins\n",
    "        self.hog = cv2.HOGDescriptor(win_size, block_size, block_stride, cell_size, num_bins)\n",
    "\n",
    "    def predict(self, img):\n",
    "        b_img, g_img, r_img = cv2.split(img)\n",
    "        grey_image = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        features_b = self.hog.compute(b_img)\n",
    "        features_g = self.hog.compute(g_img)\n",
    "        features_r = self.hog.compute(r_img)\n",
    "        features_grey = self.hog.compute(grey_image)\n",
    "\n",
    "        return np.concatenate((features_grey, features_b, features_g, features_r), axis=0)\n",
    "\n",
    "\n",
    "class sklearnHOG(FeatureExtractor):\n",
    "    def __init__(self, orientations=9, pixels_per_cell=(8, 8), cells_per_block=(2, 2), channel_axis=2):\n",
    "        super().__init__()\n",
    "        self.orientations = orientations\n",
    "        self.pixels_per_cell = pixels_per_cell\n",
    "        self.cells_per_block = cells_per_block\n",
    "        self.channel_axis = channel_axis\n",
    "\n",
    "    def predict(self, img):\n",
    "        # b_img, g_img, r_img = cv2.split(img)\n",
    "        # grey_image = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        feature = hog(img, orientations=self.orientations, pixels_per_cell=self.pixels_per_cell, cells_per_block=self.cells_per_block, channel_axis=self.channel_axis)\n",
    "        # features_b = self.hog.compute(b_img)\n",
    "        # features_g = self.hog.compute(g_img)\n",
    "        # features_r = self.hog.compute(r_img)\n",
    "        # features_grey = self.hog.compute(grey_image)\n",
    "\n",
    "        return feature\n",
    "    \n",
    "\n",
    "class SIFT(FeatureExtractor):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "    def predict(self, img):\n",
    "        gray_im = color.rgb2gray(img)\n",
    "        sift = cv2.SIFT_create()\n",
    "        _, descriptors = sift.detectAndCompute((gray_im * 255).astype(\"uint8\"), None)\n",
    "\n",
    "        if descriptors is None:\n",
    "            return np.zeros((128,))\n",
    "        \n",
    "\n",
    "        return np.mean(descriptors, axis=0)\n",
    "\n",
    "\n",
    "\n",
    "class HuMoments(FeatureExtractor):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def predict(self, img):\n",
    "        img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        moments = cv2.moments(img_gray)\n",
    "        humoments = cv2.HuMoments(moments)\n",
    "        humoments = -(np.log(np.abs(humoments))) / np.log(10)\n",
    "        return humoments.flatten()\n",
    "    \n",
    "\n",
    "class EOH(FeatureExtractor):\n",
    "    def __init__(self, num_blocks=8):\n",
    "        super().__init__()\n",
    "        self.num_blocks = num_blocks\n",
    "\n",
    "    def get_sobel_filters(self):\n",
    "        # Define Sobel filters for horizontal, vertical, 45-degree, and 135-degree edges\n",
    "        return [\n",
    "            np.array([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], dtype=np.float32),  # Horizontal\n",
    "            np.array([[1, 2, 1], [0, 0, 0], [-1, -2, -1]], dtype=np.float32),  # Vertical\n",
    "            np.array([[2, 1, 0], [1, 0, -1], [0, -1, -2]], dtype=np.float32),  # 45-degree\n",
    "            np.array([[0, 1, 2], [-1, 0, 1], [-2, -1, 0]], dtype=np.float32)  # 135-degree\n",
    "        ]\n",
    "\n",
    "    def apply_filters(self, image, filters):\n",
    "        # Apply each filter to the image\n",
    "        edge_maps = [convolve2d(image, filter, mode='same', boundary='wrap') for filter in filters]\n",
    "        return edge_maps\n",
    "\n",
    "    def calculate_histograms(self, image, edge_maps):\n",
    "        # Assume the image is divided into num_blocks x num_blocks\n",
    "        block_size = image.shape[0] // self.num_blocks\n",
    "        histograms = np.zeros((self.num_blocks, self.num_blocks, len(edge_maps)), dtype=np.float32)\n",
    "\n",
    "        for i in range(self.num_blocks):\n",
    "            for j in range(self.num_blocks):\n",
    "                block = [em[i * block_size:(i + 1) * block_size, j * block_size:(j + 1) * block_size] for em in\n",
    "                         edge_maps]\n",
    "                histograms[i, j] = [np.sum(np.abs(b)) for b in block]\n",
    "\n",
    "        return histograms\n",
    "\n",
    "    def predict(self, img):\n",
    "        # Convert the image to grayscale\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Get the Sobel filters\n",
    "        filters = self.get_sobel_filters()\n",
    "\n",
    "        # Apply the filters to the image\n",
    "        edge_maps = self.apply_filters(gray, filters)\n",
    "\n",
    "        # Calculate the histograms\n",
    "        histograms = self.calculate_histograms(gray, edge_maps)\n",
    "\n",
    "        # normalize the histograms\n",
    "        histograms /= histograms.sum(axis=(0, 1), keepdims=True)\n",
    "\n",
    "        # Flatten the histograms\n",
    "        return histograms.flatten()\n",
    "    \n",
    "\n",
    "class LBP(FeatureExtractor):\n",
    "    def __init__(self, num_points=8, radius=1):\n",
    "        super().__init__()\n",
    "        self.num_points = num_points\n",
    "        self.radius = radius\n",
    "\n",
    "    def predict(self, img, P=8, R=1):\n",
    "        # Convert the image to grayscale as LBP works on grayscale images\n",
    "        # Convert the image to grayscale\n",
    "        gray_image = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Compute the LBP features\n",
    "        lbp_image = local_binary_pattern(gray_image, 8, 1, method='uniform')\n",
    "\n",
    "        # Compute the histogram of LBP features\n",
    "        histogram, _ = np.histogram(lbp_image.ravel(), bins=np.arange(0, 60), range=(0, 59))\n",
    "\n",
    "        # Normalize the histogram\n",
    "        histogram = histogram.astype(np.float32)\n",
    "        histogram /= (histogram.sum() + 1e-5)\n",
    "\n",
    "        return histogram\n",
    "    \n",
    "\n",
    "class cooccurrence(FeatureExtractor):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "    def predict(self, img):\n",
    "        # Convert the image to grayscale\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        co_matrix = skimage.feature.graycomatrix(img, [5], [0], levels=256, symmetric=True, normed=True)\n",
    "\n",
    "        # Calculate texture features from the co-occurrence matrix\n",
    "        contrast = skimage.feature.graycoprops(co_matrix, 'contrast')\n",
    "        correlation = skimage.feature.graycoprops(co_matrix, 'correlation')\n",
    "        energy = skimage.feature.graycoprops(co_matrix, 'energy')\n",
    "        homogeneity = skimage.feature.graycoprops(co_matrix, 'homogeneity')\n",
    "\n",
    "        return np.array([contrast[0, 0], correlation[0, 0], energy[0, 0], homogeneity[0, 0]]).flatten()\n",
    "    \n",
    "\n",
    "class Gabor(FeatureExtractor):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def predict(self, img):\n",
    "        # Convert the image to grayscale\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Define the range of orientations\n",
    "        orientations = [0, np.pi / 4, np.pi / 2, 3 * np.pi / 4]\n",
    "\n",
    "        # Define the range of frequencies\n",
    "        frequencies = [0.1, 0.4, 0.7]\n",
    "\n",
    "        # Create a Gabor filter for each orientation and frequency\n",
    "        filters = [cv2.getGaborKernel((21, 21), 4.0, theta, freq, 0.5, 0, ktype=cv2.CV_32F) for theta in orientations for freq in frequencies]\n",
    "\n",
    "        # Apply each filter to the image\n",
    "        responses = [cv2.filter2D(img, cv2.CV_32F, filter) for filter in filters]\n",
    "\n",
    "        # Calculate the mean and standard deviation of the responses\n",
    "        mean = np.array([response.mean() for response in responses])\n",
    "        std = np.array([response.std() for response in responses])\n",
    "\n",
    "        return np.concatenate((mean, std)).flatten()\n",
    "    \n",
    "\n",
    "class Tamura(FeatureExtractor):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def predict(self, img):\n",
    "        # Convert the image to grayscale\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Calculate the coarseness, contrast, and directionality\n",
    "        coarseness = self.calculate_coarseness(img)\n",
    "        contrast = self.calculate_contrast(img)\n",
    "        directionality = self.calculate_directionality(img)\n",
    "\n",
    "        return np.array([coarseness, contrast, directionality]).flatten()\n",
    "\n",
    "    def calculate_coarseness(self, img):\n",
    "        # Calculate the gradient of the image\n",
    "        gradient = cv2.Sobel(img, cv2.CV_64F, 1, 1, ksize=5)\n",
    "\n",
    "        # Calculate the mean of the gradient\n",
    "        mean = np.abs(gradient).mean()\n",
    "\n",
    "        # Calculate the coarseness\n",
    "        coarseness = mean / 4.0\n",
    "\n",
    "        return coarseness\n",
    "\n",
    "    def calculate_contrast(self, img):\n",
    "        # Calculate the contrast\n",
    "        contrast = img.std()\n",
    "\n",
    "        return contrast\n",
    "\n",
    "    def calculate_directionality(self, img):\n",
    "        # Calculate the gradient of the image\n",
    "        gradient = cv2.Sobel(img, cv2.CV_64F, 1, 1, ksize=5)\n",
    "\n",
    "        # Calculate the histogram of the gradient\n",
    "        histogram, _ = np.histogram(gradient.ravel(), bins=256, range=(-128, 128))\n",
    "\n",
    "        # Normalize the histogram\n",
    "        histogram = histogram.astype(np.float32)\n",
    "        histogram /= (histogram.sum() + 1e-5)\n",
    "\n",
    "        # Calculate the entropy of the histogram\n",
    "        entropy = -np.sum(histogram * np.log2(histogram + 1e-5))\n",
    "\n",
    "        # Calculate the directionality\n",
    "        directionality = 1 - 1 / (1 + entropy)\n",
    "\n",
    "        return directionality\n",
    "\n",
    "\n",
    "class Daubechies_Wavelets(FeatureExtractor):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def predict(self, img):\n",
    "        # Convert the image to grayscale\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Perform the Daubechies wavelet transform\n",
    "        coeffs = pywt.dwt2(img, 'db1')\n",
    "\n",
    "        # Calculate the mean and standard deviation of the coefficients\n",
    "        mean = [np.mean(c) for c in coeffs]\n",
    "        std = [np.std(c) for c in coeffs]\n",
    "\n",
    "        return np.concatenate((mean, std)).flatten()\n",
    "    \n",
    "\n",
    "class ZernikeMoments(FeatureExtractor):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def predict(self, img):\n",
    "        # Convert the image to grayscale\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Calculate the Zernike moments\n",
    "        moments = mahotas.features.zernike_moments(img, radius=21, degree=8)\n",
    "\n",
    "        return moments.flatten()\n",
    "    \n",
    "\n",
    "class Haralick(FeatureExtractor):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def predict(self, img):\n",
    "        # Convert the image to grayscale\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Calculate the Haralick texture features\n",
    "        features = mahotas.features.haralick(img).mean(axis=0)\n",
    "\n",
    "        return features.flatten()\n",
    "    \n",
    "\n",
    "class GIST(FeatureExtractor):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def predict(self, img):\n",
    "        # Convert the image to grayscale\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Calculate the GIST features\n",
    "        features = mahotas.features.haralick(img).mean(axis=0)\n",
    "\n",
    "        return features.flatten()\n",
    "    \n",
    "\n",
    "class DenseSIFT(FeatureExtractor):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def predict(self, img):\n",
    "        gray_im = color.rgb2gray(img)\n",
    "        sift = cv2.SIFT_create()\n",
    "        _, descriptors = sift.detectAndCompute((gray_im * 255).astype(\"uint8\"), None)\n",
    "\n",
    "        if descriptors is None:\n",
    "            return np.zeros((128,))\n",
    "        \n",
    "\n",
    "        return np.mean(descriptors, axis=0)\n",
    "     \n",
    "\n",
    "class DCT(FeatureExtractor):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def predict(self, img):\n",
    "        # Convert the image to grayscale\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Perform the 2D DCT\n",
    "        dct = cv2.dct(np.float32(img))\n",
    "\n",
    "        return dct.flatten()\n",
    "\n",
    "\n",
    "def random_rotate(img, degrees_range=(-10, 10)):\n",
    "    angle = random.uniform(degrees_range[0], degrees_range[1])\n",
    "    height, width = img.shape[:2]\n",
    "    rotation_matrix = cv2.getRotationMatrix2D((width / 2, height / 2), angle, 1)\n",
    "    rotated_image = cv2.warpAffine(img, rotation_matrix, (width, height))\n",
    "    return rotated_image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Training Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "begin data augmentation\n",
      "100000\n",
      "data augmentation done\n",
      "110000\n",
      "finished loading\n",
      "Number of images with label 0 : 20020\n",
      "Number of images with label 1 : 10024\n",
      "Number of images with label 2 : 10076\n",
      "Number of images with label 3 : 10014\n",
      "Number of images with label 4 : 9990\n",
      "Number of images with label 5 : 9986\n",
      "Number of images with label 6 : 9910\n",
      "Number of images with label 7 : 10000\n",
      "Number of images with label 8 : 10040\n",
      "Number of images with label 9 : 9940\n"
     ]
    }
   ],
   "source": [
    "image_names = []\n",
    "labels = []\n",
    "test_image_names = []\n",
    "test_labels = []\n",
    "\n",
    "train_csv_file = './train.csv'\n",
    "test_csv_file = './test.csv'\n",
    "\n",
    "with open(train_csv_file, 'r') as file:\n",
    "    csv_reader = csv.reader(file)\n",
    "    next(csv_reader)\n",
    "    for row in csv_reader:\n",
    "        image_names.append(row[0])\n",
    "        labels.append(int(row[1]))\n",
    "\n",
    "\n",
    "imgs = []\n",
    "for i, image_name in enumerate(image_names):\n",
    "    images_path = f'./train_ims/{image_name}'\n",
    "    img = cv2.imread(images_path)\n",
    "    imgs.append(img)\n",
    "print(len(imgs))\n",
    "\n",
    "print(\"begin data augmentation\")\n",
    "for i in range(len(imgs)):\n",
    "    img = imgs[i]\n",
    "    # flip\n",
    "    img = cv2.flip(img, 1)\n",
    "    imgs.append(img)\n",
    "    labels.append(labels[i])\n",
    "    # # rotate\n",
    "    # img = random_rotate(img)\n",
    "    # imgs.append(img)\n",
    "    # labels.append(labels[i])\n",
    "print(len(imgs))\n",
    "\n",
    "print(\"data augmentation done\")\n",
    "\n",
    "with open(test_csv_file, 'r') as file:\n",
    "    csv_reader = csv.reader(file)\n",
    "    next(csv_reader)\n",
    "    for row in csv_reader:\n",
    "        test_image_names.append(row[0])\n",
    "        labels.append(int(row[1]))\n",
    "\n",
    "\n",
    "for i, image_name in enumerate(test_image_names):\n",
    "    images_path = f'./test_ims/{image_name}'\n",
    "    img = cv2.imread(images_path)\n",
    "    imgs.append(img)\n",
    "print(len(imgs))\n",
    "\n",
    "print(\"finished loading\")\n",
    "\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    print(\"Number of images with label\", i, \":\", labels.count(i))\n",
    "\n",
    "# data = list(zip(imgs, labels))\n",
    "# random.shuffle(data)\n",
    "# imgs, labels = zip(*data)\n",
    "\n",
    "imgs = np.array(imgs)\n",
    "labels = np.array(labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skHOG_extractor = sklearnHOG()\n",
    "# cvHOG_extractor = cv2HOG()\n",
    "# Color_extractor = Color()\n",
    "# Raw_extractor = Raw()\n",
    "# smoothRaw_extractor = SmoothRaw()\n",
    "# EOH_extractor = EOH()\n",
    "# HuMoments_extractor = HuMoments()\n",
    "# SIFT_extractor = SIFT()\n",
    "# LBP_extractor = LBP()\n",
    "# cooccurrence_extractor = cooccurrence()\n",
    "# Gabor_extractor = Gabor()\n",
    "# Tamura_extractor = Tamura()\n",
    "# Daubechies_Wavelets_extractor = Daubechies_Wavelets()\n",
    "# ZernikeMoments_extractor = ZernikeMoments()\n",
    "# Haralick_extractor = Haralick()\n",
    "# GIST_extractor = GIST()\n",
    "# DenseSIFT_extractor = DenseSIFT()\n",
    "# DCT_extractor = DCT()\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "# cvHOG_feature = cvHOG_extractor.featureExtract(imgs)\n",
    "# Color_feature = Color_extractor.featureExtract(imgs)\n",
    "# raw_feature = Raw_extractor.featureExtract(imgs)\n",
    "# EOH_feature = EOH_extractor.featureExtract(imgs)\n",
    "# HuMoments_feature = HuMoments_extractor.featureExtract(imgs)\n",
    "# SIFT_feature = SIFT_extractor.featureExtract(imgs)\n",
    "# LBP_feature = LBP_extractor.featureExtract(imgs)\n",
    "# cooccurrence_feature = cooccurrence_extractor.featureExtract(imgs)\n",
    "# Gabor_feature = Gabor_extractor.featureExtract(imgs)\n",
    "# Tamura_feature = Tamura_extractor.featureExtract(imgs)\n",
    "# Daubechies_Wavelets_feature = Daubechies_Wavelets_extractor.featureExtract(imgs)\n",
    "# print(Daubechies_Wavelets_feature.shape)\n",
    "# ZernikeMoments_feature = ZernikeMoments_extractor.featureExtract(imgs)\n",
    "# print(ZernikeMoments_feature.shape)\n",
    "# Haralick_feature = Haralick_extractor.featureExtract(imgs)\n",
    "# print(Haralick_feature.shape)\n",
    "# GIST_feature = GIST_extractor.featureExtract(imgs)\n",
    "# print(GIST_feature.shape)\n",
    "# DenseSIFT_feature = DenseSIFT_extractor.featureExtract(imgs)\n",
    "# print(DenseSIFT_feature.shape)\n",
    "\n",
    "# DCT_feature = DCT_extractor.featureExtract(imgs)\n",
    "# print(DCT_feature.shape)\n",
    "\n",
    "\n",
    "\n",
    "# np.savez(\"features.npz\", cvHOG_feature=cvHOG_feature, Color_feature=Color_feature, raw_feature=raw_feature, EOH_feature=EOH_feature, HuMoments_feature=HuMoments_feature, SIFT_feature=SIFT_feature, LBP_feature=LBP_feature, Gabor_feature=Gabor_feature, Tamura_feature=Tamura_feature)\n",
    "\n",
    "# append more features on top of the features.npz file\n",
    "# features = np.load(\"features.npz\")\n",
    "# cvHOG_feature = features['cvHOG_feature']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature cvHOG_feature appended (110000, 2880)\n",
      "Feature Color_feature appended (110000, 64)\n",
      "Feature raw_feature appended (110000, 3072)\n",
      "Feature EOH_feature appended (110000, 256)\n",
      "Feature HuMoments_feature appended (110000, 7)\n",
      "Feature SIFT_feature appended (110000, 128)\n",
      "Feature LBP_feature appended (110000, 59)\n",
      "Feature Gabor_feature appended (110000, 24)\n",
      "Feature Tamura_feature appended (110000, 3)\n",
      "Feature cooccurrence_feature appended (110000, 4)\n",
      "Feature Daubechies_Wavelets_feature appended (110000, 4)\n",
      "Feature ZernikeMoments_feature appended (110000, 25)\n",
      "Feature Haralick_feature appended (110000, 13)\n",
      "Feature GIST_feature appended (110000, 13)\n",
      "Total features appended: 14\n"
     ]
    }
   ],
   "source": [
    "features_file = np.load(\"features.npz\")\n",
    "\n",
    "imgs_features = np.empty((imgs.shape[0], 0))\n",
    "\n",
    "features_count = 0\n",
    "\n",
    "for features in features_file.files[:-1]:\n",
    "    imgs_features = np.concatenate((imgs_features, features_file[features]), axis=1)\n",
    "    print(f\"Feature {features} appended {features_file[features].shape}\")\n",
    "    features_count += 1\n",
    "\n",
    "print(f\"Total features appended: {features_count}\")\n",
    "# imgs_features = np.concatenate((imgs_features, Daubechies_Wavelets_feature, ZernikeMoments_feature, Haralick_feature, GIST_feature, DenseSIFT_feature), axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features_file = np.load(\"features.npz\")\n",
    "# # cvHOG_feature = features_file['cvHOG_feature']\n",
    "# # Color_feature = features_file['Color_feature']\n",
    "# # raw_feature = features_file['raw_feature']\n",
    "# # EOH_feature = features_file['EOH_feature']\n",
    "# # HuMoments_feature = features_file['HuMoments_feature']\n",
    "# # SIFT_feature = features_file['SIFT_feature']\n",
    "# # LBP_feature = features_file['LBP_feature']\n",
    "# Gabor_feature = features_file['Gabor_feature']\n",
    "# Tamura_feature = features_file['Tamura_feature']\n",
    "# cooccurrence_feature = features_file['cooccurrence_feature']\n",
    "# Daubechies_Wavelets_feature = features_file['Daubechies_Wavelets_feature']\n",
    "# ZernikeMoments_feature = features_file['ZernikeMoments_feature']\n",
    "# Haralick_feature = features_file['Haralick_feature']\n",
    "# GIST_feature = features_file['GIST_feature']\n",
    "# # DenseSIFT_feature = features_file['DenseSIFT_feature']\n",
    "# # DCT_feature = features_file['DCT_feature']\n",
    "\n",
    "# np.savez(\"sba.npz\", Gabor_feature=Gabor_feature, Tamura_feature=Tamura_feature, cooccurrence_feature=cooccurrence_feature, Daubechies_Wavelets_feature=Daubechies_Wavelets_feature, ZernikeMoments_feature=ZernikeMoments_feature, Haralick_feature=Haralick_feature, GIST_feature=GIST_feature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(110000, 6552)\n"
     ]
    }
   ],
   "source": [
    "print(imgs_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reduce Dimension**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin pca\n",
      "pca done\n",
      "(110000, 472)\n"
     ]
    }
   ],
   "source": [
    "print(\"begin pca\")\n",
    "pca = PCA(0.88, copy=False)\n",
    "pca.fit(imgs_features)\n",
    "imgs_features = pca.fit_transform(imgs_features)\n",
    "print(\"pca done\")\n",
    "\n",
    "# skHOG_feature = skHOG_extractor.featureExtract(imgs)\n",
    "# Color_feature = Color_extractor.featureExtract(imgs)\n",
    "# print(Color_feature.shape)\n",
    "# smoothRaw_feature = smoothRaw_extractor.featureExtract(imgs)\n",
    "# SIFT_feature = SIFT_extractor.featureExtract(imgs)\n",
    "\n",
    "# print(HuMoments_feature.shape)\n",
    "# LBP_feature = LBP_extractor.featureExtract(imgs)\n",
    "# print(LBP_feature.shape)\n",
    "# cooccurrence_feature = cooccurrence_extractor.featureExtract(imgs)\n",
    "# print(cooccurrence_feature.shape)\n",
    "# imgs_features = np.concatenate((imgs_features, EOH_feature, Color_feature, HuMoments_feature), axis=1)\n",
    "# pca = PCA(n_components=400)\n",
    "# pca.fit(imgs_features)\n",
    "# imgs_features = pca.fit_transform(imgs_features)\n",
    "# print(\"pca done\")\n",
    "print(imgs_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savez(\"pca_features.npz\", imgs_features=imgs_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_images = imgs_features[5000:]\n",
    "# train_labels = labels[5000:]\n",
    "# test_images = imgs_features[:5000]\n",
    "# test_labels = labels[:5000]\n",
    "train_images, test_images, train_labels, test_labels = train_test_split(\n",
    "    imgs_features[:50000], labels[:50000], test_size=0.1, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = SVC(kernel='rbf', C=5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model2 = KNeighborsClassifier(n_neighbors=10, p=1, metric='minkowski', weights='distance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model3 = LogisticRegression(max_iter=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensemble_model = VotingClassifier(estimators=[('svm', model1), ('knn', model2), ('lr', model3)], voting='soft')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load existing features from file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load features\n",
    "features_file = np.load(\"pca_features.npz\")\n",
    "imgs_features = features_file['imgs_features']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train's Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(C=5, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(C=5, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(C=5, random_state=42)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images = np.concatenate((train_images, imgs_features[50000:100000]), axis=0)\n",
    "train_labels = np.concatenate((train_labels, labels[50000:100000]), axis=0)\n",
    "model1.fit(train_images, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.798\n"
     ]
    }
   ],
   "source": [
    "train_predictions = model1.predict(test_images)\n",
    "accuracy = accuracy_score(test_labels, train_predictions)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model1.predict(imgs_features[100000:])\n",
    "\n",
    "# Create a dataframe with the image names and predictions\n",
    "result_data = pd.DataFrame({'im_name': test_image_names, 'label': predictions})\n",
    "\n",
    "# Save the result to a CSV file\n",
    "output_file_path = \"predict.csv\"\n",
    "result_data.to_csv(output_file_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "works",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
